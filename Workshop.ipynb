{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Customer segmentation with unsupervised learning\n",
    "\n",
    "__We will learn by doing__: you should complete with code the places marked with __[WORKSHOP]__\n",
    "\n",
    "We will be using the following tools in Python 3:\n",
    "* jupyter\n",
    "* pandas\n",
    "* matplotlib\n",
    "* scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## jupyter\n",
    "You are already using __jupyter__!\n",
    "\n",
    "__[jupyter](http://jupyter.org/) is a web-interface IDE__, a development environment __with REPL__ where you can execute arbitrary code and see the results instantly. Beneath the web interface there is a server that executes the code. The coding language is usually Python 3, but there are server's kerners for other languages.\n",
    "\n",
    "Thanks to its REPL _powers_, __jupyter has became the facto standard format__ when showing data analysis, research papers and all data-related presentations. For us developers we could find jupyter as a too-simple IDE, but this simplicity is actually what helps researchers to __easily explore datasets__ and try things out quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### jupyter notebook's cells\n",
    "\n",
    "__A cell usually contains code or markdown__. Do a single click here to see it ;-)\n",
    "\n",
    "Here you have some ways to move around and execute a cell:\n",
    "* You can edit any cell just clicking on it with the mouse, or use the keyboard arrows and do _Enter_\n",
    "* You can unfocus a cell with _Esc_\n",
    "* You can add a new cell with the key B (bellow) or \n",
    "* Executing a cell is simple: _shift+Enter_ or _control+Enter_ will run the code or render the markdown\n",
    "\n",
    "You can use the mouse and the top menu to do all kind of things, or learn the keyboard shortcuts (in the Help menu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This is a cell with code. Try to execute it: use the mouse to focus it and press control+Enter.\n",
    "\n",
    "' '.join(['Hello', 'world'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## pandas\n",
    "\n",
    "__[pandas](http://pandas.pydata.org/) is a library for manipulating data frames__. It is based in a smaller library, __Numpy__, which operates with matrices. But pandas leverages numpy giving an incredible collection of functions to play with the data.\n",
    "\n",
    "Let's load ulabox's dataset and play a bit with the data. If the .csv file is not in the current directory, this code will download it.\n",
    "\n",
    "In order to know __the meaning of each column__ of the dataset, please have a look at [its data dictionary](https://github.com/ulabox/datasets/blob/master/README.md).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Usually 'pandas' is nicknamed as 'pd'.\n",
    "# Please execute this cell with control+Enter (and the following ones, while you read them).\n",
    "import pandas as pd\n",
    "import os.path\n",
    "\n",
    "filename = 'ulabox_orders_with_categories_partials_2017.csv'\n",
    "if not os.path.isfile(filename):\n",
    "    import urllib.request\n",
    "    urllib.request.urlretrieve('https://raw.githubusercontent.com/ulabox/datasets/master/data/ulabox_orders_with_categories_partials_2017.csv', filename) \n",
    "\n",
    "raw_df = pd.read_csv(filename)   \n",
    "\n",
    "# head() shows first 5 rows\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you have a look at the raw data, __each row has an index__ (left, bold) __with its corresponding column data__. In data analysis rows are usually named \"samples\" while columns are named \"features\".\n",
    "\n",
    "Actually in this case __the feature \"order\" (order number) could be directly used as the index__ of the dataframe. So let's use it and then drop the original \"order\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = raw_df.reindex(index=raw_df['order'])\n",
    "\n",
    "df.drop('order', axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notice that __pandas library is really powerfull__. It can manipulate data in different ways and allows all kind of dataframe operations (at cell level, row and column level, and even between dataframes).\n",
    "\n",
    "For example, it can use multi-indexes as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "multi_indexed_df = raw_df.groupby(by=['customer','order']).sum()\n",
    "\n",
    "multi_indexed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Filtering, sampling and __indexing by sample or a feature__ is really easy too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get 50 random rows\n",
    "sample = df.sample(50, random_state = 1)\n",
    "# I use random_state to get same results (it's the random seed)\n",
    "\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Getting a column (feature)\n",
    "column = sample['total_items']\n",
    "\n",
    "column.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Getting a row (sample)\n",
    "sample.loc[8856]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Getting an individual value\n",
    "sample.loc[8856, 'hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filtering with direct test\n",
    "orders_with_more_than_50_items = sample.loc[sample['total_items'] > 50]\n",
    "\n",
    "orders_with_more_than_50_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Notice the content of the previous comparison\n",
    "(sample['total_items'] > 50).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# [WORKSHOP] Can you find any order with only Drink products bought?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You should have found more than one order with just drinks. Somebody was really thirsty!\n",
    "\n",
    "Moreover __pandas have several functions for doing statistics__, helpfull when exploring the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As you can see, the dataset contains 30k rows (samples). We get the mean, standard deviation, min, max and other statistics for each feature.\n",
    "\n",
    "Wait a moment! __Look at the discount% column__. It seems the maximum _discount%_ is 100%, and the minimum _discount%_ is (minus) -65%!! __This looks weird...__\n",
    "\n",
    "If you want to have a look at other pandas features, I recommend [its documentation](https://pandas.pydata.org/pandas-docs/stable/) and [this cheatsheet](https://github.com/pandas-dev/pandas/blob/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## matplotlib\n",
    "__[matplotlib](https://matplotlib.org/) is a library for graphs__. It can use numpy arrays and pandas dataframes as input.\n",
    "\n",
    "Actually pandas comes with direct helpers to display data thru matplotlib! If you want to learn more about matplotlib integration in pandas, check [pandas visualization documentation](https://pandas.pydata.org/pandas-docs/stable/visualization.html) and [matplotlib documentation](https://matplotlib.org/contents.html).\n",
    "\n",
    "For instance, let's explore the discount% feature using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a jupyter helper, so when a matplotlib is evaluated, it shows the graph\n",
    "%matplotlib inline\n",
    "\n",
    "# Use 15 blocks(bins)\n",
    "df['discount%'].hist(bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "After seeing the discount% feature displayed, it's clear that if the __discount% is 100%__, it should be some kind of __free order__ (like a gift for a VIP).\n",
    "\n",
    "On the other hand, why are there some negative discount% values? This is not easy to understand until you ask the domain expert: some drinks have a surcharge (a negative discount) due to a law that taxes drinks with added sugar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# [WORKSHOP] Plot a histogram to display the most common hours of the day orders are purchased. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The results should show a peak at 12~13h and one at 22h."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## scikit-learn\n",
    "\n",
    "__[scikit-learn](http://scikit-learn.org/stable/) is a library with machine learning algorithms__ and helpers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Before starting using ML algorithms, let's __prepare a smaller dataset without free orders__, with just 100 rows, so the algorithms will work really fast. Also consider only the 8 categories' partials.\n",
    "\n",
    "Notice that algorithms are optimized to work better with values between 0 to 1; __it's vital to keep values at this order of magnitude__. So an easy way to normalize the samples' data is to divide the percents values by 100. Another option could be using Standarization (see [documentation](http://scikit-learn.org/stable/modules/preprocessing.html])).\n",
    "\n",
    "In ML jargon, the algorithm input is called __X__ (a matrix with samples and features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "no_free_orders = df.loc[df['discount%'] < 100]\n",
    "one_thousand = no_free_orders.sample(100, random_state = 1)\n",
    "X = one_thousand[['Food%', 'Fresh%', 'Drinks%', 'Home%', 'Beauty%', 'Health%', 'Baby%', 'Pets%']].divide(100)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### KMeans clustering\n",
    "\n",
    "scikit-learn comes with a k-means clustering algorithm with only one mandatory parameter: the number of expected clusters. Let's try with 7 clusters.\n",
    "\n",
    "All algorithms in this library come with a .fit_predict() method to do the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "seven_clusters_alg = KMeans(n_clusters = 7, random_state = 1)\n",
    "cluster_labels = seven_clusters_alg.fit_predict(X)\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Each one of the 100 samples is classified in a cluster (from 0 to 6). For instance, the first sample felt in cluster #3, the second in #4, etc.\n",
    "\n",
    "Let's see how many samples felt in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# cluster_labels is a numpy array, so first we embed it in a dataframe and then plot each cluster counting\n",
    "pd.DataFrame(cluster_labels).hist(bins = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As you see, cluster #1 and #3 have quite a lot of samples, while cluster #5 has just 5 samples.\n",
    "\n",
    "Was choosing 7 clusters a good idea? If a cluster has a lot of samples, is it a correct clusterization? __How can we find the most \"correct\" amount of clusters?__ Silhouette score to the rescue!\n",
    "\n",
    "scikit-learn comes with a __[silhouette_score function](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html)__, that evaluates how _well_ the samples felt in the clusters. Its result is a score, the higher value the better. If you want to understand this function better, have a look at [the visual example that comes with its documentation](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html).\n",
    "\n",
    "In our case, let's do a small script to try different amount of clusters, from 2 to 20, to see the score of each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "range_n_clusters = range(2,20)\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    cluster_alg = KMeans(n_clusters = n_clusters, random_state = 1)\n",
    "    cluster_labels = cluster_alg.fit_predict(X)\n",
    "\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"the average silhouette_score is :\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Apparently choosing 7 clusters was a quite good guess, but the best score goes for 6 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# [WORKSHOP] Use KMeans algorithm with 6 clusters (and random_state = 1), and plot clusters' histogram (with 6 bins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Changing from 7 clusters to 6 doesn't look like a big improvement, but the fact that some clusters have more samples than others doesn't mean those were bad chosen. It's normal that some _kind of customers_ are more popular that _other kind of customers_.\n",
    "\n",
    "A good way to understand better those 6 clusters is to see their centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "six_clusters_alg = KMeans(n_clusters = 6, random_state = 1)\n",
    "cluster_labels = six_clusters_alg.fit_predict(X)\n",
    "\n",
    "centers = pd.DataFrame(six_clusters_alg.cluster_centers_, columns=X.columns)\n",
    "centers.multiply(100).round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__Looking at the 6 clusters' centers everything makes more sense__. Let's see what it is bought in each one:\n",
    "* #0 : food and some drinks, \"_the food lover_\"\n",
    "* #1 : drinks and some food, \"_the thirsty_\"\n",
    "* #2 : basically baby stuff, \"_the parent_\"\n",
    "* #3 : fresh, food and drink, \"_the healthy_\", very common\n",
    "* #4 : home products, \"_the cleaner_\"\n",
    "* #5 : a bit of everything, \"_the balanced_\", quite common\n",
    "\n",
    "Notice too that some features are totally irrelevant: Health% and Pets%. So we can consider ignoring them in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### DBSCAN clustering\n",
    "\n",
    "Another algorithm for clustering is DBSCAN. Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "# Let's ask for at least 3 samples in each cluster, with a maximum of 0.3 distance\n",
    "dbscan_alg = DBSCAN(eps = 0.3, min_samples = 3)\n",
    "cluster_labels = dbscan_alg.fit_predict(X)\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Ouch! This algorithm only found 3 clusters (#0, #1 and #2) and some samples are marked as outsiders (#-1).\n",
    "\n",
    "Let's try to remove Health% and Pets% as we have seen those features are irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X2 = X[['Food%', 'Fresh%', 'Drinks%', 'Home%', 'Baby%']]\n",
    "cluster_labels = dbscan_alg.fit_predict(X2)\n",
    "cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Well, the result has improved a bit. But this still has space for improvement..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# [WORKSHOP] Feel free to try alternative configurations for this algorithm..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Thank you for attending this workshop!\n",
    "\n",
    "As you have experienced, playing with Machine Learning algorithms is a question of spending time learning about the data and finding the correct parameters. Also notice that we were working with just 100 samples... things can get slow with real _big data_.\n",
    "\n",
    "I hope you liked this workshop!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
